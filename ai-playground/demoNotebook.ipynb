{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db60cb4",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8441d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from agents import Agent, Runner, trace, function_tool\n",
    "from typing import Dict\n",
    "import asyncio\n",
    "import gradio as gr\n",
    "import json\n",
    "import httpx\n",
    "# ruff: noqa: F704"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835ee59",
   "metadata": {},
   "source": [
    "## Load OpenAI Key\n",
    "\n",
    "This key is used to call OpenAI models. You can use any other model as well.\n",
    "\n",
    "At the end it is just a API call with appropriate credentianls. \n",
    "\n",
    "This SDK and frameworks are not tied or coupled to one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8adea1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cee8c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa7c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PROXY = True  # Set to True to use a SOCKS5 proxy, False otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc2ca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS: 401\n",
      "{\n",
      "  \"error\": {\n",
      "    \"message\": \"Missing bearer authentication in header\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if USE_PROXY:\n",
    "    SOCKS_PROXY = \"socks5h://127.0.0.1:9999\"\n",
    "\n",
    "    try:\n",
    "        with httpx.Client(proxy=SOCKS_PROXY, timeout=10) as c:\n",
    "            r = c.get(\"https://api.openai.com/v1/models\")\n",
    "            print(\"STATUS:\", r.status_code)\n",
    "            print(r.text[:200])\n",
    "    except Exception as e:\n",
    "        print(\"ERROR:\", type(e).__name__, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9ddec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e3ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PROXY:\n",
    "    http_client = httpx.Client(\n",
    "        proxy=SOCKS_PROXY,\n",
    "        timeout=30.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb8c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PROXY:\n",
    "    client = OpenAI(\n",
    "        http_client=http_client\n",
    "        )\n",
    "else:\n",
    "    client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a4e69",
   "metadata": {},
   "source": [
    "## Example of using any other model or proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a5c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_router_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "# if not open_router_api_key:\n",
    "#     print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "# elif not open_router_api_key.startswith(\"sk-or-v1-\"):\n",
    "#     print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "# elif open_router_api_key.strip() != open_router_api_key:\n",
    "#     print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "# else:\n",
    "#     print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d989fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client= OpenAI(\n",
    "#     base_url=\"https://openrouter.ai/api/v1\",\n",
    "#     api_key=open_router_api_key\n",
    "# )\n",
    "# MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cde28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75414435",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_dark_mode = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea2a28",
   "metadata": {},
   "source": [
    "## Types of prompts\n",
    "\n",
    "\n",
    "Models like GPT4o have been trained to receive instructions in a particular way.\n",
    "\n",
    "They expect to receive:\n",
    "\n",
    "**System prompt** -- tells them what task they are performing and what tone they should use\n",
    "\n",
    "**User prompt** -- the conversation starter that they should reply to \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d39a976",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "The API from OpenAI expects to receive messages in a particular structure.\n",
    "Many of the other APIs share this structure:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message goes here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user message goes here\"}\n",
    "]\n",
    "```\n",
    "To give you a preview, the next 2 cells make a rather simple call - we won't stretch the mighty GPT (yet!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb78383",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a snarky assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb3ccd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, let me dust off my calculator for this one... Itâ€™s 4. Shocking, I know!\n"
     ]
    }
   ],
   "source": [
    "# To give you a preview -- calling OpenAI with system and user messages:\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b78e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a polite assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45bcd2",
   "metadata": {},
   "source": [
    "## Flight AI Assistant\n",
    "\n",
    "Let us work with an example of Flight AI Assistant of a fictitious airline company.\n",
    "\n",
    "We will take this example to demostrate the concepts we learn.\n",
    "\n",
    "We will create some simple use cases just to keep the same example going. Helps to understand things better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3af6cf",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "We already saw how memory is important for maintaining conversations and giving LLM some sort of context on what is going on\n",
    "\n",
    "The SDKs have many ways to implement this, including persistent memory based on user sessions, and authentication.\n",
    "\n",
    "Here we look into the very simple way of implementing memory. SDKs have more sophisticated ways of doing this.\n",
    "\n",
    "At the end, you need to provide the memory in form of context to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += (\"Always be accurate. If you don't know the answer, say so. Do not make up an answer \"\n",
    "                  \"or attempt to use previous knowledge apart from what is provided to you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d0485",
   "metadata": {},
   "source": [
    "**Without Adding the History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    print(\"\\nðŸ’¬ New message:\", message)\n",
    "    print(\"ðŸ§  History:\", history)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + [{\"role\": \"user\", \"content\": message}]\n",
    "    print(\"\\nðŸ“¨ Final Message:\", messages)\n",
    "    response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\",js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115f1c0",
   "metadata": {},
   "source": [
    "**Adding the History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    print(\"\\nðŸ’¬ New message:\", message)\n",
    "    print(\"ðŸ§  History:\", history)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    print(\"\\nðŸ“¨ Final Message:\", messages)\n",
    "    response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\",js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623d4e8",
   "metadata": {},
   "source": [
    "## TOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d517d7",
   "metadata": {},
   "source": [
    "We saw that tools are a way to provide the LLM with capabilities\n",
    "\n",
    "But Remember, tools have to be executed by host. LLMs don't do that\n",
    "\n",
    "They can tell you when to execute and how to execute only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5443f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += (\"Always be accurate. If you don't know the answer, say so. Do not make up an answer \"\n",
    "                  \"or attempt to use previous knowledge apart from what is provided to you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    print(\"\\nðŸ’¬ New message:\", message)\n",
    "    print(\"ðŸ§  History:\", history)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    print(\"\\nðŸ“¨ Final Message:\", messages)\n",
    "    response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\",js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bebf97",
   "metadata": {},
   "source": [
    "We are creating a dummy function that returns the price of a ticket to the 4 cities below\n",
    "\n",
    "- London: USD 799\n",
    "- Paris: USD 899\n",
    "- Tokyo : USD 1400\n",
    "- Berlin: USD 499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ticket_price(\"Berlin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd5969",
   "metadata": {},
   "source": [
    "Now we will define a python object that will represent this tool.\n",
    "\n",
    "Look carefully at the structure of the object\n",
    "\n",
    "It has a name, description, parameters required along with their types and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ccfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\",\n",
    "    \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8bfe17",
   "metadata": {},
   "source": [
    "You have to create this tools array that containes the tool objects. You can have multiple tools\n",
    "\n",
    "See that the `price_function` is the definition of the function that we defined and LLM will tell you if this tool needs to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this is included in a list of tools:\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": price_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = client.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        print(\"\\nOpenAI Decides: \\n\", response.choices[0].finish_reason)\n",
    "        message = response.choices[0].message\n",
    "        print(\"\\nOpenAI Describes: \\n\", response.choices[0].message)\n",
    "        response, city = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        print(\"Messages after tool call: \", messages)\n",
    "        response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    \n",
    "    print(\"\\nTool Call is being handled by the host:\\n\", tool_call)\n",
    "    \n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    \n",
    "    city = arguments.get('destination_city')\n",
    "\n",
    "    ## Execute the function\n",
    "    price = get_ticket_price(city)\n",
    "    \n",
    "    print(\"\\nCreating a response for sending to the LLM\\n\")\n",
    "    \n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"destination_city\": city,\"price\": price}),\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    return response, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd27698",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\",js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3550a5",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "We say what is an agent and how it comprises of LLM, tools, memory.\n",
    "\n",
    "We also saw how an Agent loop is designed. Every SDK provides multiple ways to implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519e7cd0",
   "metadata": {},
   "source": [
    "Every SDK has a way to define tools. Here is an example of how you might define a tool in OpenAI SDK\n",
    "\n",
    "As you can see the SDK provides a decortator to define a tool. This takes care of creating all the boilerplate code for a tool definition we saw earlier\n",
    "\n",
    "***Notice the function docstring and type Annotations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e15f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "@function_tool\n",
    "def get_ticket_price(destination_city: str) -> str:\n",
    "    \"\"\"Get the price of a return ticket to the destination city. \n",
    "    Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\n",
    "    \"\"\"\n",
    "    print(f\"Tool get_ticket_price called for {destination_city} by the Agent\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6abde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_ticket_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8770499",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += (\"Always be accurate. If you don't know the answer, say so. Do not make up an answer \"\n",
    "                  \"or attempt to use previous knowledge apart from what is provided to you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756778fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"FlightAI\",\n",
    "    instructions=(\n",
    "        system_message\n",
    "    ),\n",
    "    tools=[get_ticket_price],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "with trace(\"FlightAIAgent\"):\n",
    "    result = await (Runner.run(agent, input=\"How much is the ticket price to Berlin\"))\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b52a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_async(message, history):\n",
    "    with trace(\"FlightAIChat\"):\n",
    "        print(\"\\nðŸ“¨ Agent Called with Message:\", message)\n",
    "        result = await Runner.run(agent, message)\n",
    "        return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat_async, type=\"messages\",js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd961b2",
   "metadata": {},
   "source": [
    "## Memory Management with SDKs and Frameworks\n",
    "\n",
    "Memory is provided to the agent simply by taking the history of messages and providing it as context to the Agent.\n",
    "\n",
    "\n",
    "However in reality the SDKs or Frameworks you use provide many ways to implement memory, also persistent memory based on user sessions and authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_async(message, history):\n",
    "    print(\"\\nðŸ’¬ New message:\", message)\n",
    "    print(\"ðŸ§  History:\", history)\n",
    "    items = [{\"role\":m.get(\"role\"),\"content\":m.get(\"content\")} for m in history if isinstance(m.get(\"content\"), str)]\n",
    "    items.append({\"role\": \"user\", \"content\": message})\n",
    "    print(\"\\nðŸ“¨ Final Message:\", items)\n",
    "    with trace(\"FlightAIChatMemory\"):\n",
    "        result = await Runner.run(agent, items)\n",
    "        return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b641db",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat_async, type=\"messages\",js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9d643",
   "metadata": {},
   "source": [
    "## MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8859f11",
   "metadata": {},
   "source": [
    "As we have seen MCP is way to provide Agents or LLMS with more capabilities using tools hosted by somewhere else than the Agent Environment\n",
    "\n",
    "We will first see client and server mechanism of MCP that does not involve any LLM or Agent.\n",
    "\n",
    "We will understand that it is standard similar to REST and clearly defines how a MCP client and server interact\n",
    "\n",
    "This standardizes the way All Agents are able to interact with any MCP compliant server and utilize their tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab7a0b",
   "metadata": {},
   "source": [
    "SDKs implement the MCP standard in their own way. But the underlying mechanism is the same across all implementations.\n",
    "\n",
    "`/list_tools` -- GET -- returns the list of tools the server has\n",
    "\n",
    "`/call_tool` -- POST -- call a tool with parameters\n",
    "\n",
    "Here we are using again OpenAI SDK but you can use any other SDK or framework that supports MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, trace\n",
    "from agents.mcp import MCPServerStdio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90b51c",
   "metadata": {},
   "source": [
    "Tell SDK how to connect to mcp server. Here we are using stdio transport but you can also use HTTP with SSE.\n",
    "\n",
    "In the OpenAI SDK you can use `MCPServerStdio` or `MCPServerHTTP` to connect to the server. This internally creates the MCP client and connects to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03607612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell server to launch as a subprocess\n",
    "\n",
    "params = {\"command\": \"uv\", \"args\": [\"run\", \"mcp_server.py\"]}\n",
    "async with MCPServerStdio(params=params, client_session_timeout_seconds=30) as server:\n",
    "    mcp_tools = await server.list_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c35880",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"command\": \"uv\", \"args\": [\"run\", \"mcp_server.py\"]}\n",
    "async with MCPServerStdio(params=params, client_session_timeout_seconds=30) as server:\n",
    "    price = await server.call_tool(tool_name=\"get_ticket_price\", arguments={\"destination_city\": \"Berlin\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072413b0",
   "metadata": {},
   "source": [
    "As you saw there is no LLM, it is just a way for a client and server to interact with set protocols and standards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240316cd",
   "metadata": {},
   "source": [
    "## Creating your own MCP Client\n",
    "\n",
    "\n",
    "Above we saw how MCP works without LLM or Agent using SDK. It creates a client and connects to the server.\n",
    "\n",
    "Lets see how you can create your own MCP Client.\n",
    "\n",
    "Here we are not using OpenAI SDK but creating our own MCP Client using MCP Python package from Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from contextlib import AsyncExitStack\n",
    "from mcp.types import TextContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_params = StdioServerParameters(command=\"uv\", args=[\"run\", \"mcp_server.py\"])\n",
    "async with AsyncExitStack() as stack:\n",
    "    try:\n",
    "        # Establish a stdio connection to the server using the server parameters\n",
    "        transport = await stack.enter_async_context(stdio_client(server_params))\n",
    "        read, write, *_ = transport\n",
    "        \n",
    "        # Create a client session using the read and write streams from the connection\n",
    "        session = await stack.enter_async_context(ClientSession(read, write))\n",
    "        # Initialize the session (e.g., perform handshake or setup operations)\n",
    "        await session.initialize()\n",
    "        print(\"\\nâœ… Connected to server. Checking Tools...\")\n",
    "\n",
    "        # Load the MCP tools from the connected server using the adapter function\n",
    "        response = await session.list_tools()\n",
    "\n",
    "        # Iterate over each tool and add it to the aggregated tools list\n",
    "        tools = response.tools\n",
    "        print(\"\\nâœ… Tools Discovered from MCP Server:~\\n\")\n",
    "        for tool in tools:\n",
    "            print(tool)\n",
    "        print(\"\\nâœ… Tool:\", [tool.name for tool in tools])\n",
    "\n",
    "    except Exception as e:\n",
    "      # Handle any errors that occur during connection or tool loading for the server\n",
    "      print(f\"âŒ Failed to connect to server : {e}\")\n",
    "\n",
    "    # call_tool: get_ticket_price\n",
    "    print(\"\\nCalling tool get_ticket_price from the MCP server for 'Berlin'\")\n",
    "\n",
    "    price_res = await session.call_tool(\n",
    "        name=\"get_ticket_price\",\n",
    "        arguments={\"destination_city\": \"Berlin\"},\n",
    "    )\n",
    "    price_text = next((c.text for c in (price_res.content or []) if isinstance(c, TextContent)), None)\n",
    "    print(\"\\nâœ… Received Response from tool get_ticket_price('Berlin') ->\", price_text or price_res)\n",
    "\n",
    "    #call_tool: convert_usd_to_eur (optional, if your server implements it)\n",
    "    print(f\"\\nCalling tool convert_usd_to_eur from the MCP server for {price_text}\")\n",
    "    eur_res = await session.call_tool(\n",
    "                name=\"convert_usd_to_eur\",\n",
    "                arguments={\"price_usd\": price_text, \"rate\": 0.92},\n",
    "                )\n",
    "    eur_text = next((c.text for c in (eur_res.content or []) if isinstance(c, TextContent)), None)\n",
    "    print(\"\\nâœ… Received Response from tool convert_usd_to_eur('$499', 0.92) ->\", eur_text or eur_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fdc28",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "### ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6e1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mcp import ClientSession, StdioServerParameters\n",
    "# from mcp.client.stdio import stdio_client\n",
    "# from contextlib import AsyncExitStack\n",
    "# from mcp.types import TextContent\n",
    "# from agents.mcp import MCPServerStdio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# server_params = [{\"command\": \"uv\", \"args\":[\"run\", \"mcp_server.py\"]}]\n",
    "# async with AsyncExitStack() as stack:\n",
    "#     # build & connect servers (RETURN actual server objects, not coroutines)\n",
    "#     mcp_servers = [\n",
    "#         await stack.enter_async_context(\n",
    "#             MCPServerStdio(\n",
    "#                 params=p,\n",
    "#                 name=f\"stdio-{i}\",\n",
    "#                 client_session_timeout_seconds=120,\n",
    "#                 # optional:\n",
    "#                 # cache_tools_list=True,\n",
    "#                 # use_structured_content=True,\n",
    "#             )\n",
    "#         )\n",
    "#         for i, p in enumerate(server_params)\n",
    "#     ]\n",
    "\n",
    "#     agent = Agent(\n",
    "#         name=\"FlightAI\",\n",
    "#         instructions=(\n",
    "#             system_message\n",
    "#         ),\n",
    "#         #tools=[get_ticket_price],\n",
    "#         mcp_servers=mcp_servers,\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         )\n",
    "#     with trace(\"FlightAIMCP\"):\n",
    "#         result = await (Runner.run(agent, input=\"How much is the ticket price to Berlin. And convert it to Euros too, please.\"))\n",
    "#         print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# server_params = [{\"command\": \"uv\", \"args\":[\"run\", \"mcp_server.py\"]}]\n",
    "# #async with AsyncExitStack() as stack:\n",
    "# stack = AsyncExitStack()\n",
    "# await stack.__aenter__() \n",
    "#     # build & connect servers (RETURN actual server objects, not coroutines)\n",
    "# mcp_servers = [\n",
    "#     await stack.enter_async_context(\n",
    "#         MCPServerStdio(\n",
    "#             params=server_param,\n",
    "#             name=f\"stdio-{i}\",\n",
    "#             client_session_timeout_seconds=120,\n",
    "#             # optional:\n",
    "#             # cache_tools_list=True,\n",
    "#             # use_structured_content=True,\n",
    "#         )\n",
    "#     )\n",
    "#     for i, server_param in enumerate(server_params)\n",
    "# ]\n",
    "\n",
    "# agent = Agent(\n",
    "#     name=\"FlightAI\",\n",
    "#     instructions=(\n",
    "#         system_message\n",
    "#     ),\n",
    "#     #tools=[get_ticket_price],\n",
    "#     mcp_servers=mcp_servers,\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     )\n",
    "#     #result = await (Runner.run(agent, input=\"How much is the ticket price to Berlin. And convert it to Euros too, please.\"))\n",
    "#     #print(result.final_output)\n",
    "    \n",
    "# async def chat_mcp(message, history):\n",
    "#         print(history)\n",
    "#         print(message)\n",
    "#         items = [{\"role\":m.get(\"role\"),\"content\":m.get(\"content\")} for m in history if isinstance(m.get(\"content\"), str)]\n",
    "#         items.append({\"role\": \"user\", \"content\": message})\n",
    "#         with trace(\"FlightAIMCPChat\"):\n",
    "#             result = await Runner.run(agent, items)\n",
    "#             return result.final_output\n",
    "\n",
    "# gr.ChatInterface(fn=chat_mcp, type=\"messages\",js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7021f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await stack.aclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab1befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# server_params = [{\"command\": \"uv\", \"args\":[\"run\", \"mcp_server.py\"]}]\n",
    "# #async with AsyncExitStack() as stack:\n",
    "# async with AsyncExitStack() as stack:\n",
    "#     mcp_servers = [\n",
    "#         await stack.enter_async_context(\n",
    "#             MCPServerStdio(\n",
    "#                 params=server_param,\n",
    "#                 name=f\"stdio-{i}\",\n",
    "#                 client_session_timeout_seconds=120,\n",
    "#                 # optional:\n",
    "#                 # cache_tools_list=True,\n",
    "#                 # use_structured_content=True,\n",
    "#             )\n",
    "#         )\n",
    "#         for i, server_param in enumerate(server_params)\n",
    "#     ]\n",
    "    \n",
    "#     agent = Agent(\n",
    "#         name=\"FlightAI\",\n",
    "#         instructions=(\n",
    "#             system_message\n",
    "#         ),\n",
    "#         #tools=[get_ticket_price],\n",
    "#         mcp_servers=mcp_servers,\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         )\n",
    "#         #result = await (Runner.run(agent, input=\"How much is the ticket price to Berlin. And convert it to Euros too, please.\"))\n",
    "#         #print(result.final_output)\n",
    "        \n",
    "#     async def chat_mcp(message, history):\n",
    "#             print(history)\n",
    "#             print(message)\n",
    "#             items = [{\"role\":m.get(\"role\"),\"content\":m.get(\"content\")} for m in history if isinstance(m.get(\"content\"), str)]\n",
    "#             items.append({\"role\": \"user\", \"content\": message})\n",
    "#             result = await Runner.run(agent, items)\n",
    "#             return result.final_output\n",
    "    \n",
    "#     gr.ChatInterface(fn=chat_mcp, type=\"messages\",js=force_dark_mode).launch()\n",
    "\n",
    "#     try:\n",
    "#         while True:\n",
    "#             await asyncio.sleep(3600)\n",
    "#     # except (KeyboardInterrupt, SystemExit):\n",
    "#     except:\n",
    "#         pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

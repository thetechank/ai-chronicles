{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db60cb4",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from agents import Agent, Runner, trace, function_tool\n",
    "from typing import Dict\n",
    "import asyncio\n",
    "import gradio as gr\n",
    "import json\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835ee59",
   "metadata": {},
   "source": [
    "## Load OpenAI Key\n",
    "\n",
    "This key is used to call OpenAI models. You can use any other model as well.\n",
    "\n",
    "At the end it is just a API call with appropriate credentianls. \n",
    "\n",
    "This SDK and frameworks are not tied or coupled to one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8adea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a4e69",
   "metadata": {},
   "source": [
    "## Example of using any other model or proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_router_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "# if not open_router_api_key:\n",
    "#     print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "# elif not open_router_api_key.startswith(\"sk-or-v1-\"):\n",
    "#     print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "# elif open_router_api_key.strip() != open_router_api_key:\n",
    "#     print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "# else:\n",
    "#     print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d989fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client= OpenAI(\n",
    "#     base_url=\"https://openrouter.ai/api/v1\",\n",
    "#     api_key=open_router_api_key\n",
    "# )\n",
    "# MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75414435",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_dark_mode = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea2a28",
   "metadata": {},
   "source": [
    "## Naive way of poviding additinal context to LLMs\n",
    "\n",
    "\n",
    "We can just read the files and store the context. Provide that context in prompt and then ask AI to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45bcd2",
   "metadata": {},
   "source": [
    "## Flight AI Assistant with Naive RAG\n",
    "\n",
    "Let us continue with our Flight AI Assistant of a fictitious airline company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e64282",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "\n",
    "employees = glob.glob(\"knowledgeBase/employees/*\")\n",
    "\n",
    "for employee in employees:\n",
    "    name = employee.split('/')[-1][:-3]\n",
    "    doc = \"\"\n",
    "    with open(employee, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "    context[name]=doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193237c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context[\"mahesh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = glob.glob(\"knowledgeBase/policies/*\")\n",
    "\n",
    "for policy in policies:\n",
    "    name = policy.split('/')[-1][:-3]\n",
    "    doc = \"\"\n",
    "    with open(policy, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "    context[name]=doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae82394",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an expert in answering accurate questions about airline company FlightAI\"\n",
    "system_message += \"Give brief, accurate answers. If you don't know the answer, say so.\"\n",
    "system_message += (\"Do not make anything up if you haven't been provided with relevant context \"\n",
    "                  \"or attempt to use previous knowledge apart from what is provided to you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4173f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Very Naive way of getting relevant context. Copmpare the query and find if there is any matching context. Then get that text and pass to LLM\n",
    "def get_relevant_context(message):\n",
    "    relevant_context = []\n",
    "    for context_title, context_details in context.items():\n",
    "        if context_title.lower() in message.lower():\n",
    "            relevant_context.append(context_details)\n",
    "    return relevant_context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_relevant_context(\"Who is Mahesh?\")\n",
    "get_relevant_context(\"What is your Baggage-Policy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ddcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(message):\n",
    "    relevant_context = get_relevant_context(message)\n",
    "    if relevant_context:\n",
    "        message += \"\\n\\nThe following additional context might be relevant in answering this question:\\n\\n\"\n",
    "        for relevant in relevant_context:\n",
    "            message += relevant + \"\\n\\n\"\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15808a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add_context(\"Who is Mahesh?\"))\n",
    "print(add_context(\"What is your Baggage-Policy?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a574461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    print(\"\\nðŸ’¬ New message:\", message)\n",
    "    print(\"ðŸ§  History:\", history)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
    "    message = add_context(message)\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    print(\"\\nðŸ“¨ Final Message:\", messages)\n",
    "    stream = client.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb225da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\",js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cca9d8",
   "metadata": {},
   "source": [
    "## Let us do RAG with Langchain\n",
    "\n",
    "RAG can be implemented using many frameworks. Langchain is one of the popular framework for building LLM applications.\n",
    "In addition to RAG it also provides memory management.\n",
    "\n",
    "### A sidenote on Embeddings, and \"Auto-Encoding LLMs\"\n",
    "\n",
    "We will be mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
    "\n",
    "OpenAI offers a model to do this, which we will use by calling their API with some LangChain code.\n",
    "\n",
    "This model is an example of an \"Auto-Encoding LLM\" which generates an output given a complete input.\n",
    "It's different to all the other LLMs we've discussed today, which are known as \"Auto-Regressive LLMs\", and generate future tokens based only on past context.\n",
    "\n",
    "Another example of an Auto-Encoding LLMs is BERT from Google. In addition to embedding, Auto-encoding LLMs are often used for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990b10b",
   "metadata": {},
   "source": [
    "## Chunking the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledgeBase/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf61cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdda0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccbff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f598681",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    if 'CTO' in chunk.page_content:\n",
    "        print(chunk)\n",
    "        print(\"\\n\",20*\"_______\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb37b4f",
   "metadata": {},
   "source": [
    "## Embedding the chunks and creating vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b04cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6647a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a Chroma Datastore already exists - if so, delete the collection to start from scratch\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca23740",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70789c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd1e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_embedding)\n",
    "sample_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3557c60",
   "metadata": {},
   "source": [
    "## Visualizing the Vector Store\n",
    "\n",
    "Let's take a minute to look at the documents and their embedding vectors to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c98d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['policies', 'employees', 'services', 'company'].index(t)] for t in doc_types]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa34171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=20)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75337151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42,perplexity=20)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4de69",
   "metadata": {},
   "source": [
    "## Time to use LangChain to bring it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# Alternative - if you'd like to use Ollama locally, uncomment this line instead\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory,callbacks=[StdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051210d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e63cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\",js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3dd11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
